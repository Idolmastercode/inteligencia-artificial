{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Final Pro: Clasificaci\u00f3n de Im\u00e1genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importaci\u00f3n de Librer\u00edas y Configuraci\u00f3n GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport gc\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import (\n    Input, Dense, Dropout, Flatten, BatchNormalization,\n    MaxPooling2D, Conv2D, LeakyReLU, Rescaling,\n    RandomFlip, RandomRotation, RandomZoom\n)\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping\nfrom tensorflow.keras.preprocessing import image\n\n# Limpieza preventiva de memoria\ntf.keras.backend.clear_session()\ngc.collect()\n\n# Configuraci\u00f3n de GPU para evitar errores de memoria\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"\u2705 GPU Detectada y Configurada: {gpus[0].name}\")\n    except RuntimeError as e:\n        print(e)\nelse:\n    print(\"\u26a0\ufe0f No se detect\u00f3 GPU. Se usar\u00e1 CPU (puede ser lento).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga del Dataset (Optimizado con Generadores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par\u00e1metros del Proyecto\nIMG_SIZE = 224\nBATCH_SIZE = 32  # Tama\u00f1o seguro para no saturar VRAM\nEPOCHS = 100\nINIT_LR = 1e-3\n\n# Ruta del Dataset (Ajusta si es necesario)\ndirname = os.path.join(os.getcwd(), 'Dataset')\n\nif not os.path.exists(dirname):\n    print(f\"\u274c ERROR: No encuentro la carpeta {dirname}\")\nelse:\n    print(f\"\ud83d\udcc2 Cargando im\u00e1genes desde: {dirname}\")\n\n# --- AQU\u00cd EST\u00c1 EL TRUCO ---\n# Usamos image_dataset_from_directory en lugar de cargar listas manuales.\n# Esto evita que la RAM explote.\nfull_ds = tf.keras.utils.image_dataset_from_directory(\n    dirname,\n    label_mode='categorical',\n    image_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=13\n)\n\nclass_names = full_ds.class_names\nprint(f\"\u2705 Clases encontradas: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento y Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisi\u00f3n de Datos (70% Train, 15% Val, 15% Test)\n# Se hace sobre el dataset din\u00e1mico, no sobre arrays en memoria.\n\ntotal_batches = tf.data.experimental.cardinality(full_ds).numpy()\ntrain_size = int(0.7 * total_batches)\nval_size = int(0.15 * total_batches)\ntest_size = int(0.15 * total_batches)\n\ntrain_ds = full_ds.take(train_size)\nremaining = full_ds.skip(train_size)\nval_ds = remaining.take(val_size)\ntest_ds = remaining.skip(val_size)\n\n# Optimizaci\u00f3n de Flujo (AUTOTUNE)\n# Permite cargar datos del disco mientras la GPU entrena.\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n\nprint(f\"Estructura lista: Train({train_size} batches) | Val({val_size} batches) | Test({test_size} batches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Definici\u00f3n del Modelo (Data Augmentation + 4 Bloques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definici\u00f3n del Modelo \"Tanque\" (Robusto contra basura y overfitting)\nsport_model = Sequential()\nsport_model.add(Input(shape=(IMG_SIZE, IMG_SIZE, 3)))\n\n# --- BLOQUE 1: Data Augmentation ---\n# Esto genera variaciones (rotaci\u00f3n, zoom) en tiempo real para aprender mejor.\nsport_model.add(RandomFlip(\"horizontal\"))\nsport_model.add(RandomRotation(0.1))\nsport_model.add(RandomZoom(0.1))\n\n# Normalizaci\u00f3n (Dentro del modelo para usar GPU)\nsport_model.add(Rescaling(1./255)) \n\n# --- BLOQUE 2: Extracci\u00f3n de Caracter\u00edsticas (Convoluciones) ---\n# Capa 1 (32 filtros)\nsport_model.add(Conv2D(32, kernel_size=(3, 3), activation='linear', padding='same'))\nsport_model.add(LeakyReLU(negative_slope=0.1))\nsport_model.add(BatchNormalization())\nsport_model.add(MaxPooling2D((2, 2), padding='same'))\nsport_model.add(Dropout(0.2))\n\n# Capa 2 (64 filtros)\nsport_model.add(Conv2D(64, kernel_size=(3, 3), activation='linear', padding='same'))\nsport_model.add(LeakyReLU(negative_slope=0.1))\nsport_model.add(BatchNormalization())\nsport_model.add(MaxPooling2D(pool_size=(2, 2)))\nsport_model.add(Dropout(0.3))\n\n# Capa 3 (128 filtros)\nsport_model.add(Conv2D(128, kernel_size=(3, 3), activation='linear', padding='same'))\nsport_model.add(LeakyReLU(negative_slope=0.1))\nsport_model.add(BatchNormalization())\nsport_model.add(MaxPooling2D(pool_size=(2, 2)))\nsport_model.add(Dropout(0.4))\n\n# Capa 4 (256 filtros - Detalles finos)\nsport_model.add(Conv2D(256, kernel_size=(3, 3), activation='linear', padding='same'))\nsport_model.add(LeakyReLU(negative_slope=0.1))\nsport_model.add(BatchNormalization())\nsport_model.add(MaxPooling2D(pool_size=(2, 2)))\nsport_model.add(Dropout(0.4))\n\n# --- BLOQUE 3: Clasificaci\u00f3n ---\nsport_model.add(Flatten())\nsport_model.add(Dense(128, activation='linear'))\nsport_model.add(LeakyReLU(negative_slope=0.1))\nsport_model.add(Dropout(0.5))\nsport_model.add(Dense(len(class_names), activation='softmax'))\n\nsport_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compilaci\u00f3n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilaci\u00f3n con SGD + Momentum (Para estabilidad)\nsport_model.compile(\n    loss='categorical_crossentropy',\n    optimizer=SGD(learning_rate=INIT_LR, momentum=0.9),\n    metrics=['accuracy']\n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci\u00f3n de Callbacks\n# 1. ClearMemory: Limpia RAM al final de cada \u00e9poca.\n# 2. EarlyStopping: Detiene si no mejora en 12 \u00e9pocas.\n\nclass ClearMemory(Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        gc.collect()\n        tf.keras.backend.clear_session()\n\nearly_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=12,\n    restore_best_weights=True,\n    verbose=1\n)\n\nprint(\"\ud83d\ude80 Iniciando entrenamiento...\")\nhistory = sport_model.fit(\n    train_ds, \n    epochs=EPOCHS,\n    verbose=1,\n    validation_data=val_ds,\n    callbacks=[ClearMemory(), early_stop]\n)\n\n# Guardar el modelo entrenado\nsport_model.save(\"modelo_final_cnn.h5\")\nprint(\"\ud83d\udcbe Modelo guardado exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluaci\u00f3n y Gr\u00e1ficas de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluaci\u00f3n Final en Test Set\ntest_eval = sport_model.evaluate(test_ds, verbose=0)\nprint(f'\ud83d\udcca Accuracy Final (Test): {test_eval[1]:.2%}')\nprint(f'\ud83d\udcc9 Loss Final (Test): {test_eval[0]:.4f}')\n\n# Gr\u00e1ficas de Rendimiento\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(len(acc))\n\nplt.figure(figsize=(16, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, 'bo-', label='Training Accuracy')\nplt.plot(epochs_range, val_acc, 'b-', linewidth=2, label='Validation Accuracy')\nplt.title('Precisi\u00f3n (Accuracy)')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, 'ro-', label='Training Loss')\nplt.plot(epochs_range, val_loss, 'r-', linewidth=2, label='Validation Loss')\nplt.title('P\u00e9rdida (Loss)')\nplt.legend()\nplt.grid(True)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reporte de Clasificaci\u00f3n (Precision/Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generaci\u00f3n de Reporte Detallado\nprint(\"Calculando predicciones detalladas...\")\ny_pred = []\ny_true = []\n\n# Iteramos sobre el dataset de test para obtener etiquetas reales vs predichas\nfor images, labels in test_ds:\n    preds = sport_model.predict(images, verbose=0)\n    y_pred.extend(np.argmax(preds, axis=1))\n    y_true.extend(np.argmax(labels.numpy(), axis=1))\n\nprint(\"\\n--- CLASSIFICATION REPORT ---\")\nprint(classification_report(y_true, y_pred, target_names=class_names))\n\n# Matriz de Confusi\u00f3n (Opcional)\n# cm = confusion_matrix(y_true, y_pred)\n# print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prueba / Demo en Vivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n# \ud83e\uddea ZONA DE PRUEBA INDIVIDUAL (DEMO)\n# =========================================\n# Coloca una imagen en la carpeta y pon su nombre abajo para probarla.\n\ndef probar_imagen(nombre_archivo):\n    ruta = os.path.join(os.getcwd(), nombre_archivo)\n    \n    if not os.path.exists(ruta):\n        print(f\"\u26a0\ufe0f No encuentro el archivo: {nombre_archivo}\")\n        return\n\n    # Cargar y preprocesar\n    img = image.load_img(ruta, target_size=(IMG_SIZE, IMG_SIZE))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0) # Batch de 1\n\n    # Predicci\u00f3n\n    prediccion = sport_model.predict(img_array, verbose=0)\n    score = tf.nn.softmax(prediccion[0])\n    \n    clase_detectada = class_names[np.argmax(score)]\n    confianza = 100 * np.max(score)\n    \n    # Mostrar\n    plt.figure(figsize=(5, 5))\n    plt.imshow(img)\n    color_txt = 'green' if confianza > 70 else 'red'\n    plt.title(f\"{clase_detectada.upper()}\\n({confianza:.2f}%)\", color=color_txt, fontsize=14, fontweight='bold')\n    plt.axis('off')\n    plt.show()\n    print(f\"Resultado: {clase_detectada} con {confianza:.2f}% de seguridad.\")\n\n# --- \u00a1CAMBIA ESTO POR TU FOTO! ---\n# Ejemplo: probar_imagen(\"mi_perro.jpg\")\nprint(\"Usa la funci\u00f3n probar_imagen('nombre.jpg') para testear.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}